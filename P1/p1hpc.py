# -*- coding: utf-8 -*-
"""P1HPC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14r_laKaRLUqMs451XpRW4xBM4aSt0Pse

# COMPUTACIÓN DE ALTAS PRESTACIONES
## Práctica #1 - Procesamiento de datos mediante Apache Spark

* Martha María Del Toro Carballo       100486134@alumnos.uc3m.es
* Brandon Solo                         100405959@alumnos.uc3m.es
* Manuel Santiago Férnandez Arias      manuefer@inf.uc3m.es

### Inicializando Spark
"""

# !apt-get update
# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget -q https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz
# !tar xf spark-3.1.3-bin-hadoop3.2.tgz
# !pip install -q findspark

import seaborn as sns

import os
import sys
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.1.3-bin-hadoop3.2"

import findspark
findspark.init()

num_cores = sys.argv[1]

from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

conf = SparkConf().set('spark.ui.port', '4050')
sc = SparkContext(conf=conf)
spark = SparkSession.builder.master(f'local[{num_cores}]').getOrCreate() 


from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

"""### Otras librerías a usar"""

from datetime import datetime
from time import time
import numpy as np
from csv import writer
import matplotlib.pyplot as plt
from operator import add
from pyspark.sql import functions as F
import os
import pandas as pd
from operator import add

"""### Funciones"""

cant_datos = 0
result_count = 0
tiempo_ejecucion = 0
velocidad_ejecucion = 0

def calcularRendimiento(results, cant_datos):
    t_0 = time()
    result_count = results.count() # Acción
    t_1 = time()
    tiempo_ejecucion = np.round(t_1 - t_0, 3)
    velocidad_ejecucion = round(cant_datos / tiempo_ejecucion, 2)

    return result_count , tiempo_ejecucion , velocidad_ejecucion

def interpretar_rendimiento(tiempo_ejecucion,velocidad_ejecucion,cant_datos):
    print(f'Han transcurrido {tiempo_ejecucion} segundos de ejecución.')
    print(f'Cantidad de datos procesados: {cant_datos}')
    print(f'Velocidad de procesamiento: {velocidad_ejecucion} registros/s')
  
def mostrar_grafico(data1, data2, title, cant_datos, legend = ['sql', 'rdd', 'data_frame'], n_cores = 4):
    x = [i for i in range(1,n_cores + 1)] # n_cores

    plt.figure(1, [14, 5], 150)

    plt.subplot(121)
    plt.title(title)
    plt.ylabel("Tiempo de ejecución (seg.)")
    plt.xlabel("Número de cores")
    plt.grid(True)
    plt.xticks(x)

    for y in data1:
        plt.plot(x, y)

    plt.legend(legend)

    plt.subplot(122)
    plt.title("Cantidad de registros: " + str(cant_datos))
    plt.ylabel("Registros procesados por segundo")
    plt.xlabel("Número de cores")
    plt.grid(True)
    plt.xticks(x)

    for y in data2:
        plt.plot(x, y)

    plt.legend(legend)

    plt.show()

def mostrar_resultados_rdd(results,columns,nfilas = 10):
    df = pd.DataFrame(results.take(nfilas), columns = columns)
    return df.style

def mostar_resultados_sql_df(results,nfilas = 10):
    return results.limit(nfilas).toPandas()

"""# Obtención de los datos"""

# Datos generales
path = "tripdata_2017-01.csv"
df_data = spark.read.csv(path, header=True, inferSchema=True)

path = "taxi+_zone_lookup.csv"
#df_zones = spark.read.csv(path, header=True, inferSchema=True)
df_zones = spark.read.option("delimiter", ";").csv(path, header=True, inferSchema=True)

"""# Limpieza de Datos

## Análisis previo de los datos

A continuación se hace un breve análisis de los datos
"""

df_data.select("total_amount").describe().show()

df_data.select("tip_amount").describe().show()

df_data.select("trip_distance").describe().show()

df_data.select("passenger_count").describe().show()

"""Representación de la dispersión y valores atípicos de propinas mediante un boxplot"""

data = df_data.select("tip_amount").rdd.flatMap(lambda x: x).collect()

# plt.boxplot(data)
# plt.show()

"""## Limpieza de datos nulos"""

# -------- Limpieza en datos generales --------
rows1 = df_data.count()

# Eliminación de nulos
df_data_filtered = df_data.dropna()

rows2 = df_data_filtered.count()

print("Cantidad de filas eliminadas: " + str(rows1 - rows2))

# -------- Limpieza en datos de zonas --------
rows1 = df_zones.count()

# Eliminación de nulos
df_zones_filtered = df_zones.dropna()

listaNulos = ["Unknown", "NA", "N/A"]

rdd_df_filtered = df_zones_filtered.rdd.filter(lambda x: x.Borough not in listaNulos)     \
                                       .filter(lambda x: x.Zone not in listaNulos)        \
                                       .filter(lambda x: x.service_zone not in listaNulos)

df_zones_filtered = spark.createDataFrame(rdd_df_filtered)
rows2 = df_zones_filtered.count()

print("Cantidad de filas eliminadas: " + str(rows1 - rows2))

"""## Eliminación de valores sin sentido lógico

A partir del análisis previo de los datos se detectaron valores negativos o iguales a 0. Por tanto, se procede a eliminar las filas que contienen:
- Propinas negativas
- Distancia de viajes negativa o igual a 0
- Coste del viaje negativo o igual a 0
- Tiempos sin sentido (llegada antes o igual que salida)
"""

rows1 = df_data_filtered.count()

# Filtrando propinas, importes negativos y tiempos
rdd_df_filtered = df_data_filtered.rdd.filter(lambda x: x.tip_amount >= 0) \
                                      .filter(lambda x: x.trip_distance > 0) \
                                      .filter(lambda x: x.total_amount >= 0) \
                                      .filter(lambda x: x.passenger_count > 0)

df_data_filtered = spark.createDataFrame(rdd_df_filtered)
rows2 = df_data_filtered.count()

print("Cantidad de filas eliminadas: " + str(rows1 - rows2))

"""## Filtrado de los valores atípicos"""

quantiles = df_data_filtered.approxQuantile("tip_amount", [0.25, 0.75], 0.0)
Q1 = quantiles[0]
Q3 = quantiles[1]
IQR = Q3 - Q1
upperRange = Q3 + 3*IQR
rdd_df_filtered = df_data_filtered.rdd.filter(lambda x: x.tip_amount < upperRange)

df_data_filtered = spark.createDataFrame(rdd_df_filtered)
rows2 = df_data_filtered.count()

print("Cantidad de filas eliminadas: " + str(rows1 - rows2))

"""Representación de la dispersión resultante del filtrado de valores atípicos teniendo en cuenta las propinas mediante un boxplot"""

data = df_data_filtered.select("tip_amount").rdd.flatMap(lambda x: x).collect()

# plt.boxplot(data)
# plt.show()

"""## Conversión del formato de tiempo"""

from pyspark.sql.functions import to_timestamp

df_data_final = df_data_filtered.withColumn("tpep_pickup_datetime", to_timestamp("tpep_pickup_datetime"))
df_data_final = df_data_final.withColumn("tpep_dropoff_datetime", to_timestamp("tpep_dropoff_datetime"))

"""Ahora que tenemos las columnas de Pickup y Dropoff formateadas a *timestamp*, se filtran los datos buscando un orden lógico de fechas:

*   El tiempo de llegada es mayor que el de salida
*   Los tiempos son diferentes
"""

df_data_final = df_data_final.rdd.filter(lambda x: (x.tpep_dropoff_datetime - x.tpep_pickup_datetime).seconds > 0)
df_data_final = spark.createDataFrame(df_data_final)

"""## Join con los datos de las zonas"""

# ------- JOIN PULocation con la tabla de zonas -------
df_data_j = df_data_final.join(df_zones_filtered, on=[df_data_final.PULocationID == df_zones_filtered.LocationID], how = 'inner')

# Eliminación de columna redundante 
df_data_j = df_data_j.drop("LocationID")

# Renombrado de las nuevas columnas  
df_data_j = df_data_j.withColumnRenamed("Borough","PUBorough")
df_data_j = df_data_j.withColumnRenamed("Zone","PUZone")
df_data_j = df_data_j.withColumnRenamed("service_zone","PUService_zone")

# ------- JOIN DOLocation con la tabla de zonas -------
df_data_j = df_data_j.join(df_zones_filtered, df_data_final.DOLocationID == df_zones_filtered.LocationID, how='inner')

# Eliminación de columna redundante 
df_data_j = df_data_j.drop("LocationID")

# Renombrado de las nuevas columnas  
df_data_j = df_data_j.withColumnRenamed("Borough","DOBorough")
df_data_j = df_data_j.withColumnRenamed("Zone","DOZone")
df_data_j = df_data_j.withColumnRenamed("service_zone","DOService_zone")

cant_datos = df_data_j.count()

"""# Enunciado de la práctica

Realice los estudios que considere necesarios sobre los datos que contiene los registros de la compañía YellowCab de Nueva York en término de viajes en taxi. Es obligatorio proporcionar al menos tres estudios. Ejemplos de estudios:

1. Velocidad media de los taxis en función de la hora.
2. Viajes en taxi más comunes
3. Registros financieros (propinas, personas, etc.)

Será necesario realizar al menos una ejecución basada en una consulta SQL (spark.sql(.....)), al menos una consulta mediante una concatenación de llamadas a métodos del DataFrame como: select, groupBy, y finalmente una haciendo uso de RDDs. Con el objetivo de estudiar el mejor método de trabajo, uno de los estudios realizados tiene que estar implementado de las tres formas.

Tendrá que preparar un informe, indicando los siguientes aspectos:
*   Tiempo de ejecución.
*   Cantidad de datos procesados en término de número de viajes.
*   Velocidad de procesamiento.

## Consulta #1: Velocidad media de los taxis en función de la hora

##### **RDD**

Para obtener el resultado deseado se creó una función velocidad_media que se aplicará al conjunto de datos con map (rdd).
"""

def velocidad_media(x):
    distancia = x.trip_distance
    t_destino = x.tpep_dropoff_datetime
    t_origen = x.tpep_pickup_datetime

    tiempo = (t_destino - t_origen).seconds / 3600

    return distancia / tiempo

results1 = df_data_j.rdd.map(velocidad_media)

"""Rendimiento:"""

result_count1 , tiempo_ejecucion1 , velocidad_ejecucion1 =  calcularRendimiento(results1, cant_datos)
interpretar_rendimiento(tiempo_ejecucion1, velocidad_ejecucion1, cant_datos)

mostrar_resultados_rdd(results1,['Velocidad media en millas por hora'])

"""## Consulta #2: Viajes en taxi más comunes

##### **SQL**

Esta consulta obtiene los viajes en taxi más comunes en función del destino y del origen de la zona de Nueva York. Esta consulta se realizó mediante SQL y se ordenó la salida descendentemente para hacer un top 10 de los viajes más comunes.
"""

df_data_j.createOrReplaceTempView('data')
results2 = spark.sql("SELECT COUNT(*) AS CANTIDAD_VIAJES, PUZone, DOZone \
                     FROM data \
                     GROUP BY PUZone, DOZone \
                     ORDER BY CANTIDAD_VIAJES DESC")

"""Rendimiento:"""

result_count2, tiempo_ejecucion2, velocidad_ejecucion2 =  calcularRendimiento(results2, cant_datos)
interpretar_rendimiento(tiempo_ejecucion2 , velocidad_ejecucion2, cant_datos)

mostar_resultados_sql_df(results2)

"""##### **RDD**

Esta consulta tiene el mismo propósito que la anterior, pero se realizó mediante RDD, empleando map y reduceByKey
"""

results3 = df_data_j.rdd.map(lambda x: (str(x.PUBorough + " [" + x.PUZone + "]" + " -> " + x.PUBorough + " [" + x.DOZone + "]"), 1)) \
                       .reduceByKey(add) \
                       .sortBy(lambda x: x[1], False)

"""Rendimiento:"""

result_count3, tiempo_ejecucion3, velocidad_ejecucion3 =  calcularRendimiento(results3, cant_datos)
interpretar_rendimiento(tiempo_ejecucion3 , velocidad_ejecucion3, cant_datos)

mostrar_resultados_rdd(results3,['Trayecto', 'Cantidad de Viajes'])

"""## Consulta #3: Propina media en funcion del número de pasajeros

##### **SQL**

Para obtener el resultado deseado mediante SQL agrupamos el resultado por el número de pasajeros y para cada uno se realiza la media.
"""

df_data_j.createOrReplaceTempView('data')
results4 = spark.sql("SELECT passenger_count, AVG(tip_amount) \
                     FROM data \
                     GROUP BY passenger_count \
                     ORDER BY passenger_count ASC")

"""Rendimiento:"""

result_count4 , tiempo_ejecucion4 , velocidad_ejecucion4 =  calcularRendimiento(results4, cant_datos)
interpretar_rendimiento(tiempo_ejecucion4 , velocidad_ejecucion4, cant_datos)

mostar_resultados_sql_df(results4)

"""##### **RDD**

Para obtener el resultado deseado mediante RDD Se usarán las funciones de transformación map y reduceByKey mediante cual se asignará a cada clave (Passenger_count) el valor de la propina media que le corresponde a cada una.
"""

results5 = df_data_j.rdd.map(lambda x: (x.passenger_count, (x.tip_amount, 1))) \
                       .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \
                       .map(lambda x: (x[0], x[1][0] / x[1][1]))

"""Rendimiento:"""

result_count5, tiempo_ejecucion5, velocidad_ejecucion5 =  calcularRendimiento(results5, cant_datos)
interpretar_rendimiento(tiempo_ejecucion5, velocidad_ejecucion5, cant_datos)

mostrar_resultados_rdd(results5,['Número de pasajeros', 'Propina media'])

"""##### **DataFrame**

El resultado se obtiene aplicando una agrupación por el campo "passenger_count", donde con el resultado obtenido se realiza una agregación por "tip_amount" (propina) y se ordena ascendentemente por el número de pasajeros.
"""

results6 = df_data_j.groupBy("passenger_count") \
                   .agg(F.mean('tip_amount'), F.min('tip_amount'), F.max('tip_amount')) \
                   .orderBy('passenger_count')

"""Rendimiento:"""

result_count6, tiempo_ejecucion6, velocidad_ejecucion6 =  calcularRendimiento(results6, cant_datos)
interpretar_rendimiento(tiempo_ejecucion6, velocidad_ejecucion6, cant_datos)

mostar_resultados_sql_df(results6)

"""## Guardando los resultados"""

final_results_df = pd.DataFrame([
  [result_count1, tiempo_ejecucion1, velocidad_ejecucion1],
  [result_count2, tiempo_ejecucion2, velocidad_ejecucion2],
  [result_count3, tiempo_ejecucion3, velocidad_ejecucion3],
  [result_count4, tiempo_ejecucion4, velocidad_ejecucion4],
  [result_count5, tiempo_ejecucion5, velocidad_ejecucion5],
  [result_count6, tiempo_ejecucion6, velocidad_ejecucion6],
], index=["c1_rdd", "c2_sql", "c2_rdd", "c3_sql", "c3_rdd", "c3_df"], columns = ["count", "tiempo", "velocidad"])
final_results_df.index.name = "id"

final_results_df.to_csv(f"final_results/final_result_{num_cores}.csv", sep=";")
